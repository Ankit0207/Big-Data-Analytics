{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDS561lab3_2020spr(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankit0207/Big-Data-Analytics/blob/master/IDS561lab3_2020spr(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2gHtbZNFS71e"
      },
      "source": [
        "#                     Spark Data Frame Transformations and Actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ye-daO2TTlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q http://apache.mirrors.hoobly.com/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkF3nnwgThbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Untar the Spark installer\n",
        "!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n20BYmYJWhL5",
        "colab_type": "code",
        "outputId": "32c1df30-104d-458a-a015-5f4e2efec899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spark-2.4.5-bin-hadoop2.7\tspark-2.4.5-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTe7J0I-Ozyj",
        "colab_type": "text"
      },
      "source": [
        "# Install findspark - a python library to find Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5w5KMo3O8aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ALuz9-YPHNW",
        "colab_type": "text"
      },
      "source": [
        "# Set environment variables\n",
        "Set Java and Spark home based on the location where they are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P4gXyFyJpug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PswYpIzbQNaZ",
        "colab_type": "text"
      },
      "source": [
        "# Creat a local Spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FY0mR4gKOla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oOk6bVK2XFyg"
      },
      "source": [
        "# Begin of Spark Data frame Transformations Program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9KyCRG7fytO",
        "colab_type": "code",
        "outputId": "cfddd142-ee56-43fc-f0f7-37240435cb84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oejHzrlyfNBg",
        "colab_type": "code",
        "outputId": "9f2179f1-08a0-4053-9731-bc8dd1062fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#Changing the directory to the required path\n",
        "%cd /content/drive/My Drive/Big Data\n",
        "!ls "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Big Data\n",
            "Amazon_Responded_Oct05.csv  spark-2.4.5-bin-hadoop2.7\n",
            "experiment.txt\t\t    spark-2.4.5-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q9dS431PQoGl"
      },
      "source": [
        "#TASK 1 :\n",
        "\n",
        "#Create a dataframe “daily_active_users”. Find out the users who are active in at least five listed days \n",
        "\n",
        "#i.e. created posts in at least 5 days) in Amazon_Responded_Oct05.csv and save their “user_screen_name” and “user_id_str” in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrIgpmuEbYSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Big Data/Amazon_Responded_Oct05.csv\", sep=\",\", error_bad_lines=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXHH3hTnBFmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Enable Arrow-based columnar data transfers\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "\n",
        "#Then convert Pandas Dataframe to Spark Dataframe\n",
        "dfs = spark.createDataFrame(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEy_K07SDDyE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "4b068c80-c394-4bf8-e696-131091fb73f1"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import functions\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "#Modify the tweet_created_at column by taking only the date i.e. Month Day and Year\n",
        "#i.e. Tue Nov 01 01:57:25 +0000 2016 is converted to Nov 01 2016\n",
        "\n",
        "new_df = dfs.withColumn('tweet_created_at', F.concat(dfs.tweet_created_at.substr(5, 6), dfs.tweet_created_at.substr(26, 5)))\n",
        "new_df.select(\"tweet_created_at\").show()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|tweet_created_at|\n",
            "+----------------+\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "|     Nov 01 2016|\n",
            "+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJAssvD4PJU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "#Below operation is done to Find out the users who created posts in at least 5 days.\n",
        "new_df1 = new_df.groupBy(\"user_screen_name\").agg(F.countDistinct(\"tweet_created_at\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPjP7DwNlLUY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "24c21d8c-f596-4612-e3a8-0a656db766cc"
      },
      "source": [
        "#Filters the dataframe based on the users who have created atleast 5 twitter posts\n",
        "new_df2 = new_df1.filter(\"count(DISTINCT tweet_created_at) >= 5\")\n",
        "\n",
        "new_df2.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------------------------+\n",
            "|user_screen_name|count(DISTINCT tweet_created_at)|\n",
            "+----------------+--------------------------------+\n",
            "|   ItsJustMe_Rae|                               6|\n",
            "|  Green_JamesBee|                               6|\n",
            "|   That_Musician|                               5|\n",
            "| PritiYa80985097|                              10|\n",
            "|    Speechlesstx|                               5|\n",
            "|    Kevin_Ellen_|                               5|\n",
            "|      nsvaluto74|                               5|\n",
            "|        RGengage|                               9|\n",
            "| sureshmuthrotil|                               6|\n",
            "|   ajaygupta1974|                               5|\n",
            "|       Tushi_Joy|                               7|\n",
            "|      lupitasahu|                               6|\n",
            "| SardarGurvinde2|                               5|\n",
            "|        RISSHHII|                               5|\n",
            "| Rajeshk76856612|                               5|\n",
            "|  dahnamchandler|                               8|\n",
            "|    TaiKamiya101|                               5|\n",
            "|      vaibhav926|                               9|\n",
            "|    boredgirl260|                               7|\n",
            "|        amruthhr|                              13|\n",
            "+----------------+--------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXgwSvDy18I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Filter the dataframe by keeping only the users obtained in the new_df2 dataframe\n",
        "new_df3 = new_df.join(new_df2, [\"user_screen_name\"], \"right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HEgfBPmMIiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import DecimalType\n",
        "\n",
        "#Create a new Spark dataframe keeping only the below columns\n",
        "daily_active_users = new_df3.select(\"user_screen_name\", \"user_id_str\")\n",
        "\n",
        "#Below code converts the exponential column type to Numeric format i.e. 4.7E38491 to 466384851\n",
        "#DecimalType(18,0) represents 18 digits to the left of the decimal and 0 digits to the right of the decimal\n",
        "\n",
        "daily_active_users = daily_active_users.withColumn(\"user_id_str\", daily_active_users.user_id_str.cast(DecimalType(18, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g8zTPtHMhbF",
        "colab_type": "code",
        "outputId": "667ba7cf-57fe-44c5-d70c-434400a0da23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "#daily_active_users contains the user name and user id of all the users who created Twitter posts in atleast 5 days\n",
        "daily_active_users.select('*').show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+-----------+\n",
            "|user_screen_name|user_id_str|\n",
            "+----------------+-----------+\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "|  Green_JamesBee|  466384851|\n",
            "+----------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1laysnMyP_Sr"
      },
      "source": [
        "#TASK 2 :\n",
        "\n",
        "#A company would like to conduct an A/B test on Twitter. The experiment.txt file includes the user_id_str they selected as potential experiment targets. \n",
        "\n",
        "#Please create a dataframe “experiment_user” to document the select user id and whether they are active users (join the dataframe from step 1). \n",
        "\n",
        "#Then calculate the percentage of active user and print out the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsezgZfQPXas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exp_txt = pd.read_csv(\"/content/drive/My Drive/Big Data/experiment.txt\", sep=\",\", header=None,error_bad_lines=False)\n",
        "\n",
        "#Enable Arrow-based columnar data transfers\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "\n",
        "#Then convert Pandas Dataframe to Spark Dataframe\n",
        "exp_df = spark.createDataFrame(exp_txt)\n",
        "\n",
        "#Rename the column to id_str\n",
        "exp_df = exp_df.withColumnRenamed(\"0\", \"user_id_str\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64NSiNijpLfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exp_df1 = daily_active_users.join(exp_df, [\"user_id_str\"], how=\"full_outer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVTMtI5opVgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cleaned_data contains the unique user_id_str records\n",
        "cleaned_data = exp_df1.drop_duplicates(['user_id_str'])\n",
        "\n",
        "#cleaned_data contains the unique user_id_str records\n",
        "duplicate_data = exp_df1.exceptAll(cleaned_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgQOK-zY6EVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "72cc86a9-01e0-4d2c-db50-44e7ea6523bd"
      },
      "source": [
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.functions import *\n",
        "#The below code will count the number of times user_id_str is present and will give it a count in a separate count column\n",
        "count_data = exp_df1.groupBy([\"user_id_str\"]).agg(F.count(F.lit(1)).alias('count'))\n",
        "\n",
        "#This part of code will replace count value with \"yes\" if count > 1 else \"no\"\n",
        "experiment_user = count_data \\\n",
        "                             .withColumn(\"count\",\\\n",
        "                              when(count_data[\"count\"] > 1, \"yes\")\\\n",
        "                             .otherwise(\"no\"))\n",
        "\n",
        "#Changing the count column name to \"Whether_active\"                            \n",
        "experiment_user = experiment_user.withColumnRenamed(\"count\", \"Whether_active\")\n",
        "experiment_user.show()                             "
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------+\n",
            "|user_id_str|Whether_active|\n",
            "+-----------+--------------+\n",
            "|   23068174|           yes|\n",
            "|   24038624|            no|\n",
            "|   25805398|            no|\n",
            "|   38289347|           yes|\n",
            "|   39395899|           yes|\n",
            "|   54311555|            no|\n",
            "|   71308901|            no|\n",
            "|   72342053|            no|\n",
            "|   84303818|            no|\n",
            "|  124404304|            no|\n",
            "|  153994363|            no|\n",
            "|  241391028|           yes|\n",
            "|  263063213|            no|\n",
            "|  293677963|           yes|\n",
            "|  351123880|            no|\n",
            "|  517127347|            no|\n",
            "|  595548720|           yes|\n",
            "|  712609620|            no|\n",
            "|  842317388|            no|\n",
            "|  911933083|            no|\n",
            "+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e1Qks1oQ68D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d34f59c2-b596-453f-8efd-4309213ffae7"
      },
      "source": [
        "#Calculate the percentage of active user and print out the result.\n",
        "\n",
        "active = experiment_user.filter(\"Whether_active == 'yes'\")\n",
        "no_active = active.count() \n",
        "total     = experiment_user.count()\n",
        "active    = (no_active/total) * 100\n",
        "print(active)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.825118656443959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el6xaCuSTyqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Keep only the user_id_str column and remove other unneccessary columns from the below Spark dataframe\n",
        "\n",
        "daily_active_users = daily_active_users.drop(\"user_screen_name\")\n",
        "daily_active_users = daily_active_users.withColumn(\"active\", lit(\"active\"))\n",
        "daily_active_users = daily_active_users.drop_duplicates([\"user_id_str\"])\n",
        "\n",
        "experiment_user    = experiment_user.drop(\"Whether_active\")\n",
        "experiment_user    = experiment_user.withColumn(\"experiment\", lit(\"experiment\")) \n",
        "experiment_user    = experiment_user.drop_duplicates([\"user_id_str\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KhzMfIHMQWJs"
      },
      "source": [
        "#TASK 3  :\n",
        "\n",
        "#To help the company prepare the data, please select the records (all columns) in Amazon_Responded_Oct05.csv when a user_id_str is included in all the 2 dataframes. \n",
        "\n",
        "#For example, if the user_id_str from Amazon_Responded_Oct05.csv cannot be found in daily_active_user and experiment_user, you should skip. Save the result in a dataframe and then export it as Amazon_new.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxCISTbTsG1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Main Table : dfs\n",
        "#Table 1    : daily_active_users\n",
        "#Table 2    : experiment_user\n",
        "\n",
        "amazon = dfs.join(daily_active_users, [\"user_id_str\"], \"left\").join(experiment_user, [\"user_id_str\"], \"left\")\n",
        "amazon = amazon.filter(amazon.active.isNotNull() | amazon.experiment.isNotNull())\n",
        "amazon = amazon.drop(\"active\", \"experiment\")\n",
        "\n",
        "amazon_new = amazon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iosw2RTDgDVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Finally Export the result to a .csv file\n",
        "amazon_new.write.format(\"csv\").save(\"/content/drive/My Drive/Big Data/Amazon_new.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrF8moos8qI7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "f0678c1f-5e90-4dce-e236-ecb1a56db908"
      },
      "source": [
        "#=============================================\n",
        "#Example 3-Table join operations for practice |\n",
        "#=============================================\n",
        "\n",
        "one = spark.createDataFrame(\n",
        "    [\n",
        "        (1, 'foo'), # create your data here, be consistent in the types.\n",
        "        (1, 'bar'),\n",
        "        (3, 'choc'),\n",
        "        (4, 'lol'),\n",
        "    ],\n",
        "    ['id', 'txt'] # add your columns label here\n",
        ")\n",
        "\n",
        "two = spark.createDataFrame(\n",
        "    [\n",
        "        (13, 'foo1'), # create your data here, be consistent in the types.\n",
        "        (5, 'bar1'),\n",
        "        (1, 'choc1'),\n",
        "        (7, 'lol1'),\n",
        "    ],\n",
        "    ['id', 'txt1'] # add your columns label here\n",
        ")\n",
        "\n",
        "three = spark.createDataFrame(\n",
        "    [\n",
        "        (11, 'foo2'), # create your data here, be consistent in the types.\n",
        "        (8, 'bar2'),\n",
        "        (19, 'choc2'),\n",
        "        (3, 'lol2'),\n",
        "    ],\n",
        "    ['id', 'txt2'] # add your columns label here\n",
        ")\n",
        "\n",
        "#two = two.drop(\"txt1\")\n",
        "#three = three.drop(\"txt2\")\n",
        "\n",
        "combined = one.join(two, [\"id\"], \"left\").join(three, [\"id\"], \"left\")\n",
        "combined = combined.filter(combined.txt1.isNotNull() | combined.txt2.isNotNull())\n",
        "combined = combined.drop(\"txt1\", \"txt2\")\n",
        "combined.show()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+\n",
            "| id| txt|\n",
            "+---+----+\n",
            "|  1| foo|\n",
            "|  1| bar|\n",
            "|  3|choc|\n",
            "+---+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}